{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cc9cd9",
   "metadata": {},
   "source": [
    "# Hate Speech on Twitter\n",
    "## Deliverable 04\n",
    "## Amir ElTabakh\n",
    "## 3/22/2022\n",
    "\n",
    "Since we are working with the Elevated Product, I can only access tweets shared within the last week. So I query for `#covid19` to get a fair sample of tweets. There are only about 6,600 tweets posted with that hashtag within the last week.\n",
    "\n",
    "The entire pipeline consists of the following:\n",
    "- Scraping tweets\n",
    "- Coverting JSON object to a DataFrame\n",
    "- Selecting only important columns\n",
    "- De-Truncating the tweets: The API as is only gives me access to the first 140 characters of any tweet. Some tweets may have up to 280 tweets. To get the remaining text per tweet I have to access the API again and get the whole tweet while passing in the tweet ID.\n",
    "- Cleaning all the tweets, that is removing punctuation, hyperlinks, stopwords, and casting the string to lowercase. This improves the performance of any text processing techniques.\n",
    "- Converting the timestamp of all tweets from UTC to EST\n",
    "- Performing sentiment analysis, that is to find the positivity [0, 1], negativity [0, 1], neutrality [0, 1], and a metric called compound [-1, 1] which aggregates positivity, negativity and neutrality. I also gather polarity [-1, 1] and subjectivity [0, 1].\n",
    "- Gather the coordinates of each tweet provided the `user.location` column provided by the API. These coordinates are helpful for some geometric mapping.\n",
    "\n",
    "For 6,600 tweets this entire pipeline takes 2 hours, 36 minutes, and 28 seconds. This poses a problem, this pipeline is not scalable.\n",
    "- For 2 million tweets the pipeline will require 31.5 days of continuous running.\n",
    "- For 10 million tweets the pipeline will require 5.2 months of continuous running.\n",
    "\n",
    "In general the pipeline itself is generally quite rapid. There are just two portions that are consuming so much time, de-truncating the tweets, and collecting coordinates for each tweet. Both processes have to connect to the API and make the request for each individual tweet. For now I removed these two portions from the pipeline. Moving forward I'll be looking into ways to optimize the pipeline.\n",
    "\n",
    "The pipeline below will return everything except the tweets will not be de-truncated, that is there may be tweets that are not complete (which may affect the sentiment analysis and any text analysis to be done in the future) and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries you will need in order to run the cell below\n",
    "# You only need to run this cell once on your device\n",
    "\n",
    "!python -m pip install git+https://github.com/tweepy/tweepy@master # install tweepy\n",
    "!pip install nltk # natural language library\n",
    "!pip install vaderSentiment # sentiment analysis\n",
    "!pip install geopy # get coordinates\n",
    "!pip install geopandas # plot coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce04413",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Connecting to Twitter API\n",
    "###########################################\n",
    "\n",
    "# importing dependencies\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "\n",
    "# Twitter Keys, tokens, and secrets are saved in seperate config file on my local device\n",
    "from config import consumer_key, consumer_secret, access_token, access_secret, bearer_token\n",
    "\n",
    "# authenticate\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Getting Tweets\n",
    "###########################################\n",
    "\n",
    "# get tweets from the API\n",
    "search_query = \"#covid19 -filter:retweets\"\n",
    "\n",
    "# center of united states\n",
    "latitude = \"37.09024\"\n",
    "longitude = \"-95.712891\"\n",
    "radius = \"791mi\"\n",
    "location = f\"{latitude},{longitude},{radius}\"\n",
    "num_of_tweets = 10000\n",
    "\n",
    "tweets = tw.Cursor(api.search_tweets,\n",
    "                  q = search_query,\n",
    "                  lang = \"en\",\n",
    "                  geocode = location).items(num_of_tweets) # getting 10,000 tweets\n",
    "\n",
    "# store the API responses in a list\n",
    "tweets_copy = []\n",
    "for tweet in tweets:\n",
    "    tweets_copy.append(tweet)\n",
    "    \n",
    "print(f\"{num_of_tweets} tweets have been scraped.\")\n",
    "\n",
    "###########################################\n",
    "# Saving scraped data as a DataFrame object\n",
    "###########################################\n",
    "    \n",
    "# Lets create a dataframe for our scraped tweets\n",
    "df = pd.DataFrame(columns = tweets_copy[0]._json.keys())\n",
    "\n",
    "for i in range(len(tweets_copy)):\n",
    "    df = df.append(pd.json_normalize(tweets_copy[i]._json))\n",
    "    \n",
    "list_of_features_to_keep = ['created_at', 'id', 'id_str', 'text', 'truncated', 'entities.hashtags', 'entities.user_mentions',\n",
    "                            'source', 'user.location', 'user.id', 'user.id_str', 'user.name',\n",
    "                            'user.screen_name', 'user.description', 'user.url', \n",
    "                            'coordinates', 'place', 'contributors', 'is_quote_status', 'user.statuses_count',\n",
    "                            'user.followers_count', 'user.friends_count', 'user.listed_count', 'user.created_at',\n",
    "                            'user.favourites_count', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive'\n",
    "                            ]\n",
    "df = df[list_of_features_to_keep]\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "\n",
    "print(f\"DataFrame has been created with {len(df)} scraped tweets.\")\n",
    "\n",
    "###########################################\n",
    "# Cleaning DataFrame\n",
    "###########################################\n",
    "        \n",
    "# clean text using regex (remove hyperlines, punctuation, remove stop words and convert to lowercase)\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "tweets_cleaned = []\n",
    "\n",
    "for tweet in df['text']:\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.\\S+', \"\", tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # remove stop words\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    tweet_without_sw = [word for word in tweet_tokens if not word in stopwords]\n",
    "    tweet = \" \".join(tweet_without_sw)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # append tweet text to list of cleaned tweet texts\n",
    "    tweets_cleaned += [tweet]\n",
    "\n",
    "# convert list to Pandas Series object\n",
    "tweets_cleaned_series = pd.Series(tweets_cleaned)\n",
    "\n",
    "# Add text_cleaned as column to df\n",
    "df.insert(df.columns.get_loc(\"text\") + 1, \"text_cleaned\", tweets_cleaned_series)\n",
    "\n",
    "print(\"All tweets have been cleaned.\")\n",
    "\n",
    "\n",
    "# Convert timestamp from UTC to EST\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "def utc_to_est(tweet_timestamp):\n",
    "        '''\n",
    "        Utility function to convert timestamps from UTC to EST.\n",
    "        '''\n",
    "        \n",
    "        eastern = timezone('US/Eastern')\n",
    "        utc = timezone('UTC')\n",
    "        created_at = datetime.strptime(tweet_timestamp, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        utc_created_at = utc.localize(created_at)\n",
    "        est_created_at = utc_created_at.astimezone(eastern)\n",
    "        \n",
    "        return est_created_at\n",
    "    \n",
    "# tweet created at\n",
    "for i in range(len(df)):\n",
    "    df['created_at'][i] = utc_to_est(df['created_at'][i])\n",
    "    \n",
    "# user account created at\n",
    "for i in range(len(df)):\n",
    "    df['user.created_at'][i] = utc_to_est(df['user.created_at'][i])\n",
    "    \n",
    "print(\"Timestamps have been converted to EST.\")\n",
    "    \n",
    "###########################################\n",
    "# Performing Sentiment Analysis\n",
    "# - Generating new columns for df\n",
    "###########################################\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# calulcating polarity and subjectivity for all tweets\n",
    "df[['polarity', 'subjectivity']] = df['text_cleaned'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "for index, row in df['text_cleaned'].iteritems():\n",
    "    # using Vader to get sentiment scores\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    \n",
    "    # determine sentiment\n",
    "    if neg > pos:\n",
    "        df.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        df.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        df.loc[index, 'sentiment'] = \"neutral\"\n",
    "    \n",
    "    # populating neg, neutral, positive, and compound columns\n",
    "    df.loc[index, 'neg'] = neg\n",
    "    df.loc[index, 'neu'] = neu\n",
    "    df.loc[index, 'pos'] = pos\n",
    "    df.loc[index, 'compound'] = comp\n",
    "    \n",
    "print(\"Sentiment Analysis Complete.\")\n",
    "\n",
    "###########################################\n",
    "# output DataFrame to csv file\n",
    "###########################################\n",
    "\n",
    "filename = \"covid19_tweets_01.csv\"\n",
    "df.to_csv(filename)\n",
    "\n",
    "print(f\"File has been outputted to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
